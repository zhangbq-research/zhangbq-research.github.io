<!DOCTYPE html>
<!-- saved from url=(0040)http://artemsheludko.pw/flexible-jekyll/ -->
<html lang="en">


	
<head>
	
<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1833684faf5f254c1bb31386c5780c57";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>

        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<title>Baoquan Zhang's Homepage</title>
        <link rel="shortcut icon" href="img/Baoquan.jpg"/>
        <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
	<meta name="keywords" content="Baoquan Zhang, Harbin Institute of Technology, Shenzhen"> 
	<meta name="description" content="Baoquan Zhang's home page">
	<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
	<meta name="google-site-verification" content="yy_3iiS_X6pJdegdwitJMrH0LRLHXwpjrV9RKLXxKjg" />
	<link rel="stylesheet" href="./css/jemdoc.css">
	<title>Baoquan Zhang, Harbin Institute of Technology, Shenzhen</title>
</head>

<body>
 <div id="layout-content" style="margin-top:25px">
 <a href="https://github.com/zhangbq-research" class="github-corner"><svg width="80" height="80" viewBox="0 0 250 250"
 style="fill:#0000FF; color:#fff; position: absolute; top: 0; border: 0; right: 0;"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z">
 </path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,
 87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
 <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 
 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,
 77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,
 116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>
 <style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,
 60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px)
 {.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
	
	
	
	
<table>
	<tbody>
		<tr>
			<td width="670">
				<div id="toptitle">					
					<h1>Baoquan Zhang &nbsp 张保权 </h1><h1>
				</h1></div>

				<h3> Assistant Professor </h3>
				<p>
					<!-- 1037 Luoyu Road,<br> -->
					<!-- National Anti-counterfeit Engineering Research Center,<br> -->
					Harbin Institute of Technology, Shenzhen (HITSZ), <br>
					Shenzhen, Guangdong, China, 518055 <br>
					<br>
					Email: baoquanzhang <strong>at</strong> hit <strong>dot</strong> edu <strong>dot</strong> cn; &nbsp baoquanzhang <strong>at</strong> 
					yeah <strong>dot</strong> net
					       
				</p>
				<p>     
					
					<!-- <a href="paper/CV.pdf"><img src="img/cv_p.jpg"  height="40px" style="margin-bottom:-3px"></a> -->
<!-- 					<a href="https://github.com/zhangbq-research"><img src="img/github.jpg" height="40px" style="margin-bottom:-3px"></a> -->
					<!-- <a href="img/weichat.pdf"><img src="img/weichat.jpg"  height="40px" style="margin-bottom:-3px"></a> -->
				</p>
			</td>
			<td>
				<a href="https://zhangbq-research.github.io/"><img src="img/zhang.jpg" alt="Baoquan Zhang" border="0" width="300"></a><br>
			</td>
		</tr><tr>
	</tr>
	</tbody>
</table>
	
	



     <h2>Biography</h2>
    <div id="news-content" >
	  <span style="margin: -10px 0px 0px 0px">I come from Arun Banner (阿荣旗), Inner Mongolia (内蒙古), and currently an Assistant Professor with the School of Computer Science and Technology, Harbin Institute of Technology,
Shenzhen, China. Before that, I obtained my Ph.D. degree at the Computer Science Dept. of <a href="https://www.hitsz.edu.cn/index.html">Harbin Institute of Technology (Shenzhen)</a> under the supervision of <a href="http://faculty.hitsz.edu.cn/yeyunming">Prof. Yunming Ye </a>.       
    </div>
	    
    <div id="news-content" >
	  <span style="margin: -10px 0px 0px 0px"> </span> <strong><i>My current research interests include a series of topics: multimodal learning, knowledge-informed machine learning, multimodal few/zero-shot learning, meta learning, and spatio-temporal data mining.</i></strong>     
    </div>
	
     
      <h2 >News</h2>
    <div id="news-content" >
	  <span style="margin: -10px 0px 0px 0px">I'm hosting a special issue <a href="https://www.mdpi.com/journal/electronics/special_issues/NQX87EZL69"> "Applied Machine Learning in Data Science"</a> under the journal of <a href="https://www.mdpi.com/journal/electronics">Electronics </a>. 
		Welcome to submit manuscript! The topic include but are not limited to <strong><i>knowledge-informed machine learning, data-limited machine learning, multimodal learning, parameter-efficient fine-tuning,
and their application on data science</i></strong>.       
    </div>
    <div id = "news-content" style="margin-top: 15px">
      <li style="margin: -10px 0px 0px 5px" ><span style="color:Red">2025.02.27</span>, three corresponding papers are accepted by <strong><i> CVPR 2025 </i></strong> and <strong><i>one paper is presented as highlight (13.5%)</i></strong>.</li> 
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.12.10</span>, one corresponding author paper is accepted by <strong><i> AAAI 2025 </i></strong>. </li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.09.27</span>, Invited as guest editor of the journal <strong><i> Electronics </i></strong>. </li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.10.08</span>, one author paper is accepted by <strong><i> IEEE Transactions on Geoscience and Remote Sensing </i></strong>.</li> 
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.09.26</span>, one corresponding author paper is accepted by <strong><i> NeurIPS 2024 </i></strong>.</li> 
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.08.29</span>, one papers are accepted by <strong><i> Pattern Recognition </i></strong>.</li>    
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.02.27</span>, one corresponding author paper is accepted by <strong><i>IEEE Transactions on Geoscience and Remote Sensing</i></strong>.</li>    
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2024.02.27</span>, two papers (one first-author paper and one co-author paper) are accepted by <strong><i>CVPR 2024</i></strong>.</li>    
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.12.09</span>, one paper is accepted by <strong><i>AAAI 2024</i></strong>.</li>    
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.10.25</span>, one paper is accepted by <strong><i>IEEE/ACM Transactions on Computational Biology and Bioinformatics</i></strong>.</li>    
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.09.10</span>, one corresponding author paper is accepted by <strong><i>Neural Networks (NN)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.07.07</span>, one co-author paper is accepted by <strong><i>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.05.04</span>, one paper is accepted by <strong><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.02.28</span>, one co-author paper is accepted by <strong><i>CVPR'2023</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.02.26</span>, I have successfully passed the Ph.D. Oral Defence. You may call me Dr.Zhang from now.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2023.02.17</span>, one co-author paper is accepted by <strong><i>Neural Networks (NN)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.12.03</span>, one paper is accepted by <strong><i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.08.16</span>, one paper is accepted by <strong><i>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.07.02</span>, one co-author paper is accepted by <strong><i>Information Science</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.05.02</span>, one co-author paper is accepted by <strong><i>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.04.21</span>, one paper is accepted by <strong><i>IJCAI'2022</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.04.14</span>, one co-author paper is accepted by <strong><i>Pattern Recognition (PR)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2022.04.06</span>, one co-author paper is accepted by <strong><i>Neural Networks (NN)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2021.12.01</span>, one paper is accepted by <strong><i>AAAI'2022</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2021.03.07</span>, one paper is accepted by <strong><i>Pattern Recognition (PR)</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2021.03.01</span>, one paper is accepted by <strong><i>CVPR'2021</i></strong>.</li>
      <li style="margin: 10px 0px 0px 5px" ><span style="color:Red">2019.03.01</span>, I start my Ph.D study in ICES at <strong>HITSZ</strong>.</li>
    </div>
     
 <tr><tr><tr><tr>
<div style="margin-top: 10px"></div>
    <h2>Publications </h2>
<!--<p><a href="http://scholar.google.com/citations?user=PeMuphgAAAAJ">My Google Scholar</a></p>-->
<table id="tbPublications" width="100%">
	<tbody>
<h3 style="color: red">Preprints</h3>
	
   <tr>	
		<td width="206">
		<img src="img/metadiff.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning.</a><strong>[<a href="https://github.com/zhangbq-research/MetaDiff">Code</a>]</strong><br>
		<p ><strong>Baoquan Zhang</strong>, Demin Yu.</p>
        <p class="post-date" style="margin-top: -10px" ><i>arXiv preprint arXiv: 2307.16424</i>, 2023. <strong>
		[<a href="https://arxiv.org/abs/2307.16424">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2307.16424.pdf">PDF</a>]</strong></p>
		</td>
	</tr>
	
	<tr></tr>
    <tr></tr>
    <tr></tr>
 </tbody>
</table>
	      
<table id="tbPublications" width="100%">
<tbody>
<h3 style="color: red">Conference Papers ("*" denotes corresponding author) </h3>
<tr>
	
		<td width="206">
		<img src="img/vqct.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">Sensitivity-Aware Efficient Fine-Tuning via Compact Dynamic-Rank Adaptation.</a><strong>[<a href="https://github.com/zhangbq-research/VQCT">Code</a>]</strong><br>
		<p > Tianran Chen, Jiarui Chen, <strong>Baoquan Zhang*</strong>, ZhehaoYu, Shidong Chen, Rui Ye, Xutao Li, Yunming Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i> IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2025. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2405.14206">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2405.14206">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/vqct.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">AlphaPre: Amplitude-Phase Disentanglement Model for Precipitation Nowcasting.</a><strong>[<a href="https://github.com/zhangbq-research/VQCT">Code</a>]</strong><br>
		<p > Kenghong Lin, <strong>Baoquan Zhang*</strong>, Demin Yu, Wenzhi Feng, Shidong Chen, Feifan Gao, Xutao Li, Yunming Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i> IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2025. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2405.14206">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2405.14206">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/vqct.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">Towards Improved Text-Aligned Codebook Learning: Multi-Hierarchical Codebook-Text Alignment with Long Text (Highlight).</a><strong>[<a href="https://github.com/zhangbq-research/VQCT">Code</a>]</strong><br>
		<p > Guotao Liang, <strong>Baoquan Zhang*</strong>, Zhiyuan Wen, Junteng Zhao, Yunming Ye, kolaye, Yao He.</p>
        <p class="post-date" style="margin-top: -10px" ><i> IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2025. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2405.14206">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2405.14206">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/vqct.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">AsyncDSB: Schedule-Asynchronous Diffusion Schrödinger Bridge for Image Inpainting.</a><strong>[<a href="https://github.com/zhangbq-research/VQCT">Code</a>]</strong><br>
		<p > Zihao Han, <strong>Baoquan Zhang*</strong>, Lisai Zhang, Shanshan Feng, Kenghong Lin, Guotao Liang, Yunming Ye, Xiaochen Qi, Guangming Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i> Thirty-Sixth AAAI Conference on Artificial Intelligence (<strong> AAAI </strong>), 2025. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/pdf/2412.08149v1">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2412.08149v1">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/vqct.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">LG-VQ: Language-Guided Codebook Learning.</a><strong>[<a href="https://github.com/zhangbq-research/VQCT">Code</a>]</strong><br>
		<p > Guotao Liang, <strong>Baoquan Zhang*</strong>, Yaowei Wang, Xutao Li, Yunming Ye, Huaibin Wang, Chuyao Luo, Kola Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i> Thirty-eighth Annual Conference on Neural Information Processing Systems (<strong> NeurIPS </strong>), 2024. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2405.14206">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2405.14206">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/vqct.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">Codebook Transfer with Part-of-Speech for Vector-Quantized Image Modeling.</a><strong>[<a href="https://github.com/zhangbq-research/VQCT">Code</a>]</strong><br>
		<p ><strong>Baoquan Zhang</strong>, Wanghuaibin, Luo Chuyao, Xutao Li, Liang Guotao, Yunming Ye, joeq, Yao He .</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2403.10071">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2403.10071.pdf">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/diffcast.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">DiffCast: A Unified Framework via Residual Diffusion for Precipitation Nowcasting.</a><br>
		<p >Demin Yu, Xutao Li, Yunming Ye, <strong>Baoquan Zhang</strong>, Luo Chuyao, Kuai Dai, wangrui, Chenxunlai.</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2024. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2312.06734">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2312.06734.pdf">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/metadiff.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning.</a><strong>[<a href="https://github.com/zhangbq-research/MetaDiff">Code</a>]</strong><br>
		<p ><strong>Baoquan Zhang</strong>, Chuyao Luo, Demin Yu, Xutao Li, Huiwei Lin, Yunming Ye, and Bowen Zhang.</p>
        <p class="post-date" style="margin-top: -10px" ><i>Thirty-Sixth AAAI Conference on Artificial Intelligence (<strong> AAAI </strong>), 2024. <strong>(CCF-A)</strong></i>.
		[<a href="https://arxiv.org/abs/2307.16424">arXiv</a>] 
		[<a href="https://arxiv.org/abs/2307.16424.pdf">PDF</a>]</strong></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
<tr>
	
		<td width="206">
		<img src="img/pcr.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> PCR: Proxy-based Contrastive Replay for Online Class-Incremental Continual Learning.</a><strong>[<a href="">PDF</a>]</strong><strong>[<a href="">arXiv</a>]</strong><strong>[<a href="">Code</a>]</strong><br>
		 <p > Huiwei Lin, <strong>Baoquan Zhang</strong>, Shanshan Feng, Xutao Li, Yunming Ye*.</p>
       <p style="margin-top: -11px"><i> IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2023. <strong>(CCF-A)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>	
	
<tr>
	
		<td width="206">
		<img src="img/HyperKT.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> Hyperbolic Knowledge Transfer with Class Hierarchy for Few-Shot Learning.</a><strong>[<a href="">PDF</a>]</strong><strong>[<a href="https://www.ijcai.org/proceedings/2022/0517.pdf">arXiv</a>]</strong><strong>[<a href="https://www.ijcai.org/proceedings/2022/0517.pdf">Code</a>]</strong><br>
		 <p ><strong>Baoquan Zhang</strong>, Hao Jiang, Shanshan Feng, Xutao Li, Yunming Ye*, Rui Ye.</p>
       <p style="margin-top: -11px"><i> The 31st International Joint Conference on Artificial Intelligence (<strong> IJCAI </strong>), 2022. <strong>(CCF-A)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
	
<tr>
	
		<td width="206">
		<img src="img/MetaNODE.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> MetaNODE: Prototype Optimization as a Neural ODE for Few-Shot Learning.</a><strong>[<a href="https://arxiv.org/pdf/2103.14341.pdf">PDF</a>]</strong><strong>[<a href="https://arxiv.org/abs/2103.14341">arXiv</a>]</strong><strong>[<a href="https://github.com/zhangbq-research/metanode">Code</a>]</strong><br>
		 <p ><strong>Baoquan Zhang</strong>, Xutao Li*, Shanshan Feng, Yunming Ye*, Rui Ye.</p>
       <p style="margin-top: -11px"><i>Thirty-Sixth AAAI Conference on Artificial Intelligence (<strong> AAAI </strong>), 2022. <strong>(CCF-A)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>
	
<tr>
	
		<td width="206">
		<img src="img/PrototypeCompletionCVPR.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td> <a href="" target="_blank"> Prototype Completion with Primitive Knowledge for Few-Shot Learning.</a><strong>[<a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhang_Prototype_Completion_With_Primitive_Knowledge_for_Few-Shot_Learning_CVPR_2021_paper.pdf">PDF</a>]</strong><strong>[<a href="https://arxiv.org/pdf/2009.04960.pdf">arXiv</a>]</strong><strong>[<a href="https://github.com/zhangbq-research/Prototype_Completion_for_FSL">Code</a>]</strong><br>
		 <p ><strong>Baoquan Zhang</strong>, Xutao Li*, Yunming Ye*, Zhichao Huang, Lisai Zhang.</p>
       <p style="margin-top: -11px"><i>IEEE Conference on Computer Vision and Pattern Recognition (<strong> CVPR </strong>), 2021. <strong>(CCF-A)</strong> </i></p>
		</td>
	</tr>
	<tr></tr>
    <tr></tr>

		
<table id="tbPublications" width="100%">
<tbody>	
<h3 style="color: red">Journal Papers ("*" denotes corresponding author) </h3>
   <tr>	
		<td width="206">
		<img src="img/penet.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">LGCNet: A Cloud Detection Method in Remote Sensing Images Using Local and Global Semantics.</a><br>
		<p >Chen Luo, Shanshan Feng, Huaibin Wang, <strong>Baoquan Zhang</strong>*, Pengjuan Yao, Chuyao Luo, Yunming Ye, Yong Xu, Xutao Li, Hao Fang.</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</i>, 2024. <strong>
		[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10562225">PDF</a>]</strong></p>
		</td>
	</tr>
   <tr>	
		<td width="206">
		<img src="img/penet.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">PEPNet: A barotropic primitive equations-based network for wind speed prediction.</a><br>
		<p >Rui Ye, <strong>Baoquan Zhang</strong>*, Xutao Li, Yunming Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i>Neural Networks (NN)</i>, 2023. <strong>
		[<a href="https://www.sciencedirect.com/science/article/pii/S089360802300463X">PDF</a>]</strong></p>
		</td>
	</tr>

   <tr>	
		<td width="206">
		<img src="img/TRCDNet.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">TRCDNet: A Transformer Network for Video Cloud Detection.</a><br>
		<p >Chen Luo, Shanshan Feng, Yingling Quan, Yunming Ye, Xutao Li, Yong Xu, <strong>Baoquan Zhang</strong>, Zhihao Chen.</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</i>, 2023. <strong>
		[<a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10175078">PDF</a>]</strong></p>
		</td>
	</tr>
	
   <tr>	
		<td width="206">
		<img src="img/PrototypeCompletion.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">Prototype Completion for Few-Shot Learning.</a><br>
		<p ><strong>Baoquan Zhang</strong>, Xutao Li*, Yunming Ye*, Shanshan Feng.</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</i>, 2023. <strong>
		[<a href="https://arxiv.org/abs/2108.05010">arXiv</a>] 
		[<a href="https://arxiv.org/pdf/2108.05010.pdf">PDF</a>]</strong></p>
		</td>
	</tr>
	
   <tr>	
		<td width="206">
		<img src="img/MetaDT.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">MetaDT: Meta Decision Tree for Interpretable Few-Shot Learning.</a><br>
		<p ><strong>Baoquan Zhang</strong>, Hao Jiang, Xutao Li, Shanshan Feng, Yunming Ye*, Chen Luo, Rui Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</i>, 2022. <strong>
		[<a href="https://arxiv.org/abs/2203.01482">arXiv</a>] 
		[<a href="https://ieeexplore.ieee.org/abstract/document/10130710/">PDF</a>]</strong></p>
		</td>
	</tr>
	
   <tr>	
		<td width="206">
		<img src="img/SGMNet.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">SGMNet: Scene Graph Matching Network for Few-Shot Remote Sensing Scene Classification.</a><br>
		<p ><strong>Baoquan Zhang</strong>, Shanshan Feng, Xutao Li, Yunming Ye*, Rui Ye.</p>
        <p class="post-date" style="margin-top: -10px" ><i>IEEE Transactions on Geoscience and Remote Sensing (TGRS)</i>, 2022. <strong>
		[<a href="https://arxiv.org/abs/2110.04494">arXiv</a>] 
		[<a href="https://ieeexplore.ieee.org/document/9869702/authors#authors">PDF</a>](SCI, IF=8.125, CCF-B)</strong></p>
		</td>
	</tr>
	
    <tr>	
		<td width="206">
		<img src="img/MetaConcept.png" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">Learn to Abstract via Concept Graph for Weakly-Supervised Few-Shot Learning.</a><br>
		<p ><strong>Baoquan Zhang</strong>, Ka-Cheong Leung, Xutao Li, Yunming Ye*.</p>
	       <p style="margin-top: -11px"><i>Pattern Recognition (<strong> PR </strong>), 2021, accepted. <strong>
		 [<a href="https://arxiv.org/pdf/2007.02379.pdf">arXiv</a>] [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320321001333">PDF</a>](SCI, IF=8.518, CCF-B)</strong></p>
		</td>
	</tr>
	<tr></tr>
	
    <tr>	
		<td width="206">
		<img src="img/" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">DynamicNet: A time-variant ODE network for multi-step wind speed prediction.</a><br>
		<p >Rui Ye, Xutao Li, Yunming Ye*, <strong>Baoquan Zhang</strong>.</p>
	       <p style="margin-top: -11px"><i>Neural Networks (<strong> NN </strong>), 2022, accepted. <strong>
		 [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608022001356?casa_token=-wW7yLGLbp0AAAAA:ltDwRyfoAP2KLwXabcUiB6vW19RhMUGCMRP1t8V28UZBqhGCHcRDYexwl_7dLCy9pZRlt_yjFe8K">PDF</a>](SCI, IF=8.05, CCF-B)</strong></p>
		</td>
	</tr>
	<tr></tr>	
	
    <tr>	
		<td width="206">
		<img src="img/" width="185px" height = "95" style="box-shadow: 4px 4px 8px #888">
		</td>		
		<td><a href="" target="_blank">ECDNet: A Bilateral Lightweight Cloud Detection Network for Remote Sensing Images.</a><br>
		<p >Chen Luo, Shanshan Feng, Xutao Li, Yunming Ye*, <strong>Baoquan Zhang</strong>, Zhihao Chen, YingLing Quan.</p>
	       <p style="margin-top: -11px"><i>Pattern Recognition (<strong> PR </strong>), 2022, accepted. <strong>
		 [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320322001947">PDF</a>](SCI, IF=8.518, CCF-B)</strong></p>
		</td>
	</tr>
	<tr></tr>

    <tr></tr>
    <tr></tr>

</tbody>
</table>
	  
	   
    <div id="footer">
	<div id="footer-text"></div>
    </div>
    <div id = "logo" style="margin-top: 10px; text-align:center">
	    <div align="center" style="margin:auto;padding-top:10px">
            <div style="width:12%">
                <!-- <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=kKMhkSHMeApLgHGqTfk5xOrHScwhlrHiRwRbeYjigWg"></script> -->
				<script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=CB_0cCfqo2Ei-iIjTyRnPy5vjx3ySkIS2Oka3wH_jfA"></script>
            </div>
           <br>
        &copy; Baoquan Zhang | <span style="color:Red">Last updated: September 28 2024</span>
            </div>
     </div>     
    
  </div>

</body></html>
